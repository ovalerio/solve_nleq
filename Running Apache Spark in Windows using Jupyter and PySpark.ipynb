{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Spark from Scratch\n",
    "\n",
    "This is a tutorial in order to help people who are interested in learning the basics about Spark.  I'm creating this tutorial as I myself learn about Spark in order to document the things I found.  Hopefully some of this things could also be useful to other people out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Spark in Windows using Jupyter and PySpark\n",
    "\n",
    "There could be several reasons why you would like to install Spark in a standalone environment. In my case I'm doing it because I don't have access to a computer cluster where Spark is already running and I wanted as I mentioned before to learn the basics of it before commiting myself to a larger project.\n",
    "\n",
    "This guide is specifically for people working with Windows, which is currently my setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.  Installing Python 3 and Jupyter Notebook\n",
    "\n",
    "There are several guides out there explaining in very much detail howto install Python 3 and Jupyter. Most people agree that in case you're not an experienced software engineer or even if you are but you are lazy, the easiest way to do it is use [Anaconda Python distribution](https://www.anaconda.com/download/) which is pretty up to date and comes with a lot of useful modules already preinstalled, among them Jupyter Notebook, which is exactly what I'm using now to write this guide and what I plan to use in order to demonstrate how to launch Spark processes using Python and PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.  Installing Spark\n",
    "\n",
    "If you are like me you are probably not a fan of installing stuff in your computer. The good news is that Spark does not really has to be installed. There are precompiled binaries already available in a zip file that you can obtain from [Spark website](https://spark.apache.org/downloads.html). I recommend you to have a double look and get sure to get the package prebuilt with support for Apache Hadoop 2.7 or later. This is currently the default option offer at the website.\n",
    "\n",
    "![Download Apache Spark](download_apache_spark.png)\n",
    "\n",
    "Once you have downloaded Spark you will have to unpack it and place the files somewhere in your computer. (In my case I choose to create a directory 'spark' directly under my D:\\ drive. Since it has been packed as a .tgz file, that means you would probably need to unzip and then to untar the file. At the end you will end up with something like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Installing Hadoop Winutils\n",
    "\n",
    "Steve Loughan is one of Hadoop developers and he took the time to create Windows Binaries of Hadoop official ASF releases. The original purpose of the binaries was to test Hadoop/YARN apps on Windows. But he also kindly uploaded the binaries to [Github](https://github.com/steveloughran/winutils/).\n",
    "\n",
    "This binaries were created using Maven on top of Java 1.8, which is something important to remember when choosing a compatible version of Java to install in the next step.\n",
    "\n",
    "From all the Hadoop binaries provided in Steve's Github we just need to download [winutils.exe] (https://github.com/steveloughran/winutils/blob/master/hadoop-2.8.1/winutils.exe)\n",
    "\n",
    "After downloading this file proceed to copy it into the bin directory from Spark that we downloaded in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Installing Java\n",
    "\n",
    "Spark is itself written on Scala which in turns makes use of the Java VM to run. Therefore we need to install Java in order to run Spark. Specifically we need to install Java 1.8. You can download a [Java VM installer](https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html) directly from Oracle. You will need to create an Oracle account in order to dowload the file. \n",
    "\n",
    "![Download the Java Runtime](download_java_runtime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> In case you are not allowed to run the Oracle installer, you can download instead the OpenJDK which is \n",
    "conveniently distributed as a compressed file and can be downloaded from <a href=\"https://installbuilder.bitrock.com/java/\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After downloading and extracting the Java Runtime Enviromnent, you will need to set the **JAVA_HOME** environment variable in Windows, in order to indicate where the Java home directory has been extracted.  Additionally your will need to append the **%JAVA_HOME%/bin** directory to the Windows **PATH** environment variable. These variables can be set using the system properties window.\n",
    "\n",
    "![Setting Java Environment Variables](setting_java_home_and_path.png)\n",
    "\n",
    "After setting the environment variables is recommended to test they are indeed set and can be read from the user environment. This can be done using the **ECHO** command inside a Windows **CMD** shell window.\n",
    "\n",
    "![Reading Java Environment Variables](read_java_env_vars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Installing PySpark\n",
    "\n",
    "PySpark is the module we use in Python to created and SparkContext. PySpark is built on top of Spark's Java API. The data as can be seen from the diagram is processed in Python and cached / shuffled in the JVM.\n",
    "\n",
    "![PySpark Data Flow](pyspark_data_flow.jpeg)\n",
    "\n",
    "\n",
    "In the Python driver program, SparkContext uses [Py4J](http://py4j.sourceforge.net/) to launch a JVM and create a JavaSparkContext.\n",
    "\n",
    "Installing PySpark is pretty straightforward. It can be done using **pip** inside the *Conda Power Shell* entering the command: **pip install pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.  Install the findspark python module\n",
    "\n",
    "Since Spark is usually run in distributed environments where the Spark Context is already present, there is no need to locate it, but for our own learning purposes, where we are running Spark in our local machine, in what is known as a Standalone configuration in Spark Slang. It is necessary to first show Python where the Spark home directory has been installed. The findspark module was created for this purpose. To install **findspark** we again make use of the *Conda Power Shell*.\n",
    "\n",
    "![Installing Python Modules](installing_python_modules.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Test pyspark connection to Spark\n",
    "\n",
    "Last we would like to test our Spark setup is working. For that I created the following Jupyter Notebook which you can download and run in your own Jupyter Notebook environment.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('D:\\spark\\spark-2.4.4-bin-hadoop2.7') ## *** SET THIS ACCORDING TO YOUR SPARK HOME DIRECTORY ** ##\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.appName(\"Local Spark Context\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1572000944878'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '52958'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Local Spark Context'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', '172.16.180.180')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.180.180:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Local Spark Context</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e13cec3e48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> I have also uploaded the example above as a separate Notebook, so that you can download it and try it out in your own local environment. (<a href=\"ConnectingToSparkInJupyter.ipynb\">Connecting to Spark in Jupyter</a>)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Few years ago Chang Hsin Lee first published a guide to [Install and Run PySpark in Windows](https://changhsinlee.com/install-pyspark-windows-jupyter/). I based my own guide on his, and introduced a few advice I tought would be relevant/useful for other people working to setup a Spark development environment in Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was a little curious to find out more about the developer of the findspark module. It turns out he is an active jupyterhub developer as well as ipython and jupyter contributor. He has been a speaker at JupyterCon in 2018 talking about his experience using JupyterHub in education.\n",
    "\n",
    "[Min Ragan Kelley JupyterCon 2018 Session Transcripts](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/speaker/133185)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
